{"cells":[{"cell_type":"code","source":["import sys\n","is_colab = 'google.colab' in sys.modules"],"metadata":{"id":"JQVRm7VlmmeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sys.executable"],"metadata":{"id":"OXVVe_WLm7c5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# Attempt to mount the drive, if it fails, provide troubleshooting steps\n","try:\n","    drive.mount('/content/drive')\n","except ValueError:\n","    print(\"Drive mounting failed. Please try the following steps:\")\n","    print(\"1. Ensure you have a stable internet connection.\")\n","    print(\"2. Check your Google Drive authorization.\")\n","    print(\"3. Restart the runtime and try again.\")\n","    print(\"4. If the issue persists, search for solutions online or report the error to Google Colab support.\")"],"metadata":{"id":"naNpZrTgl4rZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCwIM4KhjnB9"},"outputs":[],"source":["!pip install altair"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VJavK_FjnB_"},"outputs":[],"source":["!pip install spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-10T11:11:29.572437Z","iopub.status.busy":"2024-09-10T11:11:29.571977Z","iopub.status.idle":"2024-09-10T11:11:33.258318Z","shell.execute_reply":"2024-09-10T11:11:33.257173Z","shell.execute_reply.started":"2024-09-10T11:11:29.572402Z"},"id":"GqjVFRhijnCA"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import warnings\n","import altair as alt\n","import pickle\n","import string\n","import spacy\n","import nltk\n","import re\n","\n","from sklearn.naive_bayes import *\n","from sklearn.ensemble import *\n","from sklearn.neighbors import *\n","from sklearn.tree import *\n","from sklearn.calibration import *\n","from sklearn.linear_model import *\n","from sklearn.multiclass import *\n","from sklearn.svm import *\n","from sklearn.neural_network import MLPClassifier\n","from xgboost import XGBClassifier\n","from nltk.stem import WordNetLemmatizer\n","from collections import Counter\n","\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, auc, roc_curve\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer, HashingVectorizer\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n","from sklearn.pipeline import Pipeline, make_pipeline\n","\n","#nltk.download('stopwords')\n","sns.set(style='whitegrid')\n","%matplotlib inline\n","warnings.filterwarnings('ignore')\n","\n","# Load data\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/overview-of-recordings.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"I1uta0s7jnCB"},"source":["# [**Exploratory Data Analysis**](http://)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:11:39.108941Z","iopub.status.busy":"2024-09-10T11:11:39.108513Z","iopub.status.idle":"2024-09-10T11:11:39.136943Z","shell.execute_reply":"2024-09-10T11:11:39.135659Z","shell.execute_reply.started":"2024-09-10T11:11:39.108900Z"},"id":"lmgVoCYVjnCD"},"outputs":[],"source":["#Analyze Data\n","def explore_data(df):\n","    print(f\"The data contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n","    print('\\n')\n","    print('Dataset columns:',df.columns)\n","    print('\\n')\n","    print(df.info())\n","\n","explore_data(df)"]},{"cell_type":"markdown","metadata":{"id":"0rTu6RWjjnCE"},"source":["# [**Checking for Nan Values and duplicates**Â¶](http://)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:11:43.677143Z","iopub.status.busy":"2024-09-10T11:11:43.676706Z","iopub.status.idle":"2024-09-10T11:11:43.689414Z","shell.execute_reply":"2024-09-10T11:11:43.688204Z","shell.execute_reply.started":"2024-09-10T11:11:43.677103Z"},"id":"E6Nq_7vKjnCE"},"outputs":[],"source":["df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:11:46.851976Z","iopub.status.busy":"2024-09-10T11:11:46.851516Z","iopub.status.idle":"2024-09-10T11:11:46.876978Z","shell.execute_reply":"2024-09-10T11:11:46.875832Z","shell.execute_reply.started":"2024-09-10T11:11:46.851936Z"},"id":"Vc3frxHOjnCG"},"outputs":[],"source":["def checking_removing_duplicates(df):\n","    count_dups = df.duplicated().sum()\n","    print(\"Number of Duplicates: \", count_dups)\n","    if count_dups >= 1:\n","        df.drop_duplicates(inplace=True)\n","        print('Duplicate values removed!')\n","    else:\n","        print('No Duplicate values')\n","checking_removing_duplicates(df)"]},{"cell_type":"markdown","metadata":{"id":"ZKR598iXjnCG"},"source":["# **Corpus**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:11:51.186376Z","iopub.status.busy":"2024-09-10T11:11:51.185955Z","iopub.status.idle":"2024-09-10T11:11:51.204123Z","shell.execute_reply":"2024-09-10T11:11:51.202828Z","shell.execute_reply.started":"2024-09-10T11:11:51.186340Z"},"id":"oOCSIoK7jnCH"},"outputs":[],"source":["df_text = df[['phrase', 'prompt']]\n","df_text"]},{"cell_type":"markdown","metadata":{"id":"I7TC5xzLjnCI"},"source":["# **Document-Term Matrix**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:11:55.365541Z","iopub.status.busy":"2024-09-10T11:11:55.365167Z","iopub.status.idle":"2024-09-10T11:11:55.523947Z","shell.execute_reply":"2024-09-10T11:11:55.522995Z","shell.execute_reply.started":"2024-09-10T11:11:55.365509Z"},"id":"N1ezs_8PjnCJ"},"outputs":[],"source":["!pip install -U scikit-learn\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","cv = CountVectorizer()\n","df_cv = cv.fit_transform(df_text.phrase)\n","# Use get_feature_names_out() instead of get_feature_names()\n","data_dtm = pd.DataFrame(df_cv.toarray(), columns=cv.get_feature_names_out())\n","data_dtm.index = df_text.index\n","data_dtm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:11:58.572711Z","iopub.status.busy":"2024-09-10T11:11:58.572321Z","iopub.status.idle":"2024-09-10T11:11:58.761180Z","shell.execute_reply":"2024-09-10T11:11:58.760136Z","shell.execute_reply.started":"2024-09-10T11:11:58.572673Z"},"id":"R-e1kvn2jnCJ"},"outputs":[],"source":["# Add features\n","# Number of characters in the text\n","df_text['phrase_length'] = df_text['phrase'].apply(len)\n","# Number of words in the text\n","df_text['phrase_num_words'] = df_text['phrase'].apply(lambda x: len(x.split()))\n","# Average length of the words in the text\n","df_text[\"mean_word_len\"] = df_text[\"phrase\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","# Number of non-stopwords in the text\n","df_text['phrase_non_stopwords'] = df_text['phrase'].apply(lambda x: len([t for t in x.split() if t not in STOP_WORDS]))\n","df_text.describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-09-10T11:12:08.410611Z","iopub.status.busy":"2024-09-10T11:12:08.410222Z","iopub.status.idle":"2024-09-10T11:12:08.446165Z","shell.execute_reply":"2024-09-10T11:12:08.444922Z","shell.execute_reply.started":"2024-09-10T11:12:08.410577Z"},"id":"2lVE2sRWjnCK"},"outputs":[],"source":["import altair as alt\n","import pandas as pd\n","\n","# Assuming 'df_text' contains your data and 'prompt' is the column with categories\n","cat_dist = df_text['prompt'].value_counts().reset_index()\n","cat_dist.columns = ['Count', 'count']  # Rename columns for clarity\n","\n","alt.Chart(cat_dist).mark_bar(opacity=0.7).encode(\n","    x=alt.X('Count', title='Count'),\n","    y=alt.Y('count', sort='-x', title='Category'),\n","    tooltip=['count', 'Count']\n",").properties(height=800, width=700, title=\"Class Distribution\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-09-10T11:12:15.381906Z","iopub.status.busy":"2024-09-10T11:12:15.381507Z","iopub.status.idle":"2024-09-10T11:12:15.389756Z","shell.execute_reply":"2024-09-10T11:12:15.388729Z","shell.execute_reply.started":"2024-09-10T11:12:15.381871Z"},"id":"fMYcYM4-jnCL"},"outputs":[],"source":["target = df_text['prompt'].values\n","counter = Counter(target)\n","for k,v in counter.items():\n","    per = v / len(target) * 100\n","    print('Class=%s, Count=%d, Percentage=%.3f%%' % (k, v, per))"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-09-10T11:12:18.867263Z","iopub.status.busy":"2024-09-10T11:12:18.866859Z","iopub.status.idle":"2024-09-10T11:12:19.095980Z","shell.execute_reply":"2024-09-10T11:12:19.094348Z","shell.execute_reply.started":"2024-09-10T11:12:18.867229Z"},"id":"-h9dOiQujnCL"},"outputs":[],"source":["alt.data_transformers.disable_max_rows()\n","alt.Chart(df_text).mark_bar(color=\"violet\",opacity=0.7,\n","    interpolate='step').encode(\n","    alt.X(\"phrase_length:Q\",  bin=alt.Bin(maxbins=100), title='Phrase Length Class'),\n","    alt.Y('count()', axis=alt.Axis(labels=False), title='Frequency'),\n","    tooltip=['phrase_length']\n",").properties(\n","    height=400,\n","    width=700, title=\"Length Distribution\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xcx0KMJujnCM"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBcJ2OnqjnCN"},"outputs":[],"source":["nltk.download('averaged_perceptron_tagger')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDcFK08xjnCO"},"outputs":[],"source":["nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YJz4fc8jnCO"},"outputs":[],"source":["nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:12:26.523061Z","iopub.status.busy":"2024-09-10T11:12:26.522657Z","iopub.status.idle":"2024-09-10T11:12:30.593847Z","shell.execute_reply":"2024-09-10T11:12:30.592772Z","shell.execute_reply.started":"2024-09-10T11:12:26.523026Z"},"id":"oE5iRF4bjnCP"},"outputs":[],"source":["def clean_txt(docs):\n","    lemmatizer = WordNetLemmatizer()\n","    # split into words\n","    speech_words = nltk.word_tokenize(docs)\n","    # convert to lower case\n","    lower_text = [w.lower() for w in speech_words]\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n","    # remove punctuation from each word\n","    stripped = [re_punc.sub('', w) for w in lower_text]\n","    # remove remaining tokens that are not alphabetic\n","    words = [word for word in stripped if word.isalpha()]\n","    # filter out stop words\n","    words = [w for w in words if not w in  list(STOP_WORDS)]\n","    # filter out short tokens\n","    words = [word for word in words if len(word) > 2]\n","    #Stemm all the words in the sentence\n","    lem_words = [lemmatizer.lemmatize(word) for word in words]\n","    combined_text = ' '.join(lem_words)\n","    return combined_text\n","\n","# Cleaning the text data\n","df_text['cleaned_phrase'] = df_text['phrase'].apply(clean_txt)\n","df_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5jh9MEGjnCP"},"outputs":[],"source":["from nltk.probability import FreqDist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cu6QIWWjnCQ"},"outputs":[],"source":["freq_splits = FreqDist(df_text['phrase'])\n","print(f\"***** 10 most common strings ***** \\n{freq_splits.most_common(10)}\", \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"E5A-glHyjnCR"},"source":["#Text Data Preparation and Model Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:12:47.141062Z","iopub.status.busy":"2024-09-10T11:12:47.140622Z","iopub.status.idle":"2024-09-10T11:12:47.161540Z","shell.execute_reply":"2024-09-10T11:12:47.160343Z","shell.execute_reply.started":"2024-09-10T11:12:47.140995Z"},"id":"TlQkW3RDjnCR"},"outputs":[],"source":["# Spot-Check Normalized Text Models\n","def NormalizedTextModel(nameOfvect):\n","    if nameOfvect == 'countvect':\n","        vectorizer = CountVectorizer()\n","    elif nameOfvect =='tfvect':\n","        vectorizer = TfidfVectorizer()\n","    elif nameOfvect == 'hashvect':\n","        vectorizer = HashingVectorizer()\n","\n","    pipelines = []\n","    pipelines.append((nameOfvect+'MultinomialNB'  , Pipeline([('Vectorizer', vectorizer),('NB'  , MultinomialNB())])))\n","    pipelines.append((nameOfvect+'CCCV' , Pipeline([('Vectorizer', vectorizer),('CCCV' , CalibratedClassifierCV())])))\n","    pipelines.append((nameOfvect+'KNN' , Pipeline([('Vectorizer', vectorizer),('KNN' , KNeighborsClassifier())])))\n","    pipelines.append((nameOfvect+'CART', Pipeline([('Vectorizer', vectorizer),('CART', DecisionTreeClassifier())])))\n","    pipelines.append((nameOfvect+'PAC'  , Pipeline([('Vectorizer', vectorizer),('PAC'  , PassiveAggressiveClassifier())])))\n","    pipelines.append((nameOfvect+'SVM' , Pipeline([('Vectorizer', vectorizer),('RC' , RidgeClassifier())])))\n","    pipelines.append((nameOfvect+'AB'  , Pipeline([('Vectorizer', vectorizer),('AB'  , AdaBoostClassifier())])  ))\n","    pipelines.append((nameOfvect+'GBM' , Pipeline([('Vectorizer', vectorizer),('GMB' , GradientBoostingClassifier())])))\n","    pipelines.append((nameOfvect+'RF'  , Pipeline([('Vectorizer', vectorizer),('RF'  , RandomForestClassifier())])))\n","    pipelines.append((nameOfvect+'ET'  , Pipeline([('Vectorizer', vectorizer),('ET'  , ExtraTreesClassifier())])))\n","    pipelines.append((nameOfvect+'SGD'  , Pipeline([('Vectorizer', vectorizer),('SGD'  , SGDClassifier())])))\n","    pipelines.append((nameOfvect+'OVRC'  , Pipeline([('Vectorizer', vectorizer),('OVRC'  , OneVsRestClassifier(LogisticRegression()))])))\n","    pipelines.append((nameOfvect+'Bagging'  , Pipeline([('Vectorizer', vectorizer),('Bagging'  , BaggingClassifier())])))\n","    pipelines.append((nameOfvect+'NN'  , Pipeline([('Vectorizer', vectorizer),('NN'  , MLPClassifier())])))\n","    #pipelines.append((nameOfvect+'xgboost', Pipeline([('Vectorizer', vectorizer), ('xgboost', XGBClassifier())])))\n","    return pipelines\n","\n","# Traing model\n","def fit_model(X_train, y_train,models):\n","    # Test options and evaluation metric\n","    num_folds = 10\n","    scoring = 'accuracy'\n","\n","    results = []\n","    names = []\n","    for name, model in models:\n","        kfold = KFold(n_splits=num_folds)\n","        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","        results.append(cv_results)\n","        names.append(name)\n","        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","        print(msg)\n","\n","# Split data to training and validation set\n","def read_in_and_split_data(data, features,target):\n","    X = data[features]\n","    y = data[target]\n","    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n","    return X_train, X_test, y_train, y_test\n","\n","X = 'cleaned_phrase'\n","target_class = 'prompt'\n","X_train, X_test, y_train, y_test = read_in_and_split_data(df_text, X, target_class)"]},{"cell_type":"markdown","metadata":{"id":"L35Rv6WYjnCX"},"source":["#Bag of Words Model\n"]},{"cell_type":"markdown","metadata":{"id":"HRHLt6rojnCc"},"source":["## [Word Counts with countvectorizer ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:12:54.155242Z","iopub.status.busy":"2024-09-10T11:12:54.154829Z","iopub.status.idle":"2024-09-10T11:12:54.165626Z","shell.execute_reply":"2024-09-10T11:12:54.164113Z","shell.execute_reply.started":"2024-09-10T11:12:54.155206Z"},"id":"TyqTDr7qjnCc"},"outputs":[],"source":["# sample text\n","sample_text_count = X_train[:10]\n","# create the transform\n","vectorizer = CountVectorizer()\n","# tokenize and build vocab\n","vectorizer.fit(sample_text_count)\n","# summarize\n","print(vectorizer.vocabulary_)\n","# encode document\n","vector = vectorizer.transform(sample_text_count)\n","# summarize encoded vector\n","print(vector.shape)\n","print(type(vector))\n","print(vector.toarray())"]},{"cell_type":"markdown","metadata":{"id":"Uw8FikVyjnCd"},"source":["# [**Spot-Check Algorithms with Countvectorizer**](http://)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T11:12:59.528275Z","iopub.status.busy":"2024-09-10T11:12:59.527849Z"},"id":"ofSQonptjnCd"},"outputs":[],"source":["# Contvectorizer\n","models = NormalizedTextModel('countvect')\n","fit_model(X_train, y_train, models)"]},{"cell_type":"markdown","metadata":{"id":"ehIBynEKjnCj"},"source":["## [**Word Frequencies with TfidfVectorizer** ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFP_VhW-jnCj"},"outputs":[],"source":["# sample text\n","sample_text_Tfid = X_train[:10]\n","# create the transform\n","vectorizer = TfidfVectorizer()\n","# tokenize and build vocab\n","vectorizer.fit(sample_text_Tfid)\n","# summarize\n","print(vectorizer.vocabulary_)\n","print(vectorizer.idf_)\n","# encode document\n","vector = vectorizer.transform(sample_text_Tfid)\n","# summarize encoded vector\n","print(vector.shape)\n","print(vector.toarray())"]},{"cell_type":"markdown","metadata":{"id":"ixnc4QYUjnCk"},"source":["# [**Spot-Check Algorithms with TfidfVectorizer**](http://)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLdd4G-sjnCp"},"outputs":[],"source":["# TfidfVectorizer\n","models = NormalizedTextModel('tfvect')\n","fit_model(X_train, y_train, models)"]},{"cell_type":"markdown","metadata":{"id":"Zs2-0P0MjnCp"},"source":["## [Hashing with HashingVectorize]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqQSKW07jnCq"},"outputs":[],"source":["# sample text\n","sample_text_hash = X_train[:10]\n","# create the transform\n","vectorizer = HashingVectorizer(n_features=20)\n","# encode document\n","vector = vectorizer.transform(sample_text_hash)\n","# summarize encoded vector\n","print(vector.shape)\n","print(vector.toarray())"]},{"cell_type":"markdown","metadata":{"id":"hHPEd1BcjnCq"},"source":["# [**Spot-Check Algorithms  with HashingVectorizer**](http://)"]},{"cell_type":"markdown","metadata":{"id":"9fsuwG8pjnCu"},"source":["# [**Fine tuning**](http://)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"099juAN_jnCu"},"outputs":[],"source":["vectorizer = TfidfVectorizer()\n","X_train_1 = vectorizer.fit_transform(X_train)\n","model = BaggingClassifier()\n","n_estimators = [10, 100, 1000]\n","#learning_rate= [0.1, 0.001, 0.0001]\n","#max_depth = [4,5,6]\n","#min_child_weight=[4,5,6]\n","\n","#define grid search\n","grid = dict(n_estimators=n_estimators)\n","cv = KFold(n_splits=10)\n","grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n","grid_result = grid_search.fit(X_train_1, y_train)\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"]},{"cell_type":"markdown","metadata":{"id":"l9ruB5JAjnCy"},"source":["# [**Predict unseen data**](http://)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pcana_uyjnCy"},"outputs":[],"source":["def classification_metrics(model, y_test, y_pred):\n","    print(f\"Training Accuracy Score: {model.score(X_train, y_train) * 100:.1f}%\")\n","    print(f\"Validation Accuracy Score: {model.score(X_test, y_test) * 100:.1f}%\")\n","\n","    conf_matrix = confusion_matrix(y_test, y_pred)\n","    fig,ax = plt.subplots(figsize=(8,6))\n","    sns.heatmap(pd.DataFrame(conf_matrix), annot = True, cmap = 'YlGnBu',fmt = 'g')\n","    ax.xaxis.set_label_position('top')\n","    plt.tight_layout()\n","    plt.title('Confusion matrix for Logisitic Regression Model', fontsize=20, y=1.1)\n","    plt.ylabel('Actual label', fontsize=15)\n","    plt.xlabel('Predicted label', fontsize=15)\n","    plt.show()\n","    print(classification_report(y_test, y_pred))\n","\n","text_clf = Pipeline([('vect', TfidfVectorizer()),('bagging', BaggingClassifier(n_estimators=10))])\n","model = text_clf.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","classification_metrics(model,y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pvCCsbUWjnC2"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from xgboost import XGBClassifier\n","from sklearn.neural_network import MLPClassifier  # Simple deep learning (multi-layer perceptron)\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","# Function to fit and evaluate models\n","def fit_model(X_train, y_train, models):\n","    num_folds = 10\n","    scoring = 'accuracy'\n","\n","    results = []\n","    names = []\n","    for name, model in models:\n","        kfold = KFold(n_splits=num_folds)\n","        # Convert y_train to numerical labels using LabelEncoder\n","        le = LabelEncoder()\n","        y_train_encoded = le.fit_transform(y_train)\n","        cv_results = cross_val_score(model, X_train, y_train_encoded, cv=kfold, scoring=scoring)\n","        results.append(cv_results)\n","        names.append(name)\n","        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","        print(msg)\n","\n","# Split data to training and validation set\n","def read_in_and_split_data(data, features, target):\n","    X = data[features]\n","    y = data[target]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","    return X_train, X_test, y_train, y_test\n","\n","# Function to train and predict with Naive Bayes\n","def naive_bayes_classifier(X_train, X_test, y_train, y_test):\n","    model = MultinomialNB()\n","    model.fit(X_train, y_train)  # Train the Naive Bayes model\n","    predictions = model.predict(X_test)  # Predict on the test set\n","    accuracy = accuracy_score(y_test, predictions)  # Evaluate accuracy\n","    print(f\"Naive Bayes Accuracy: {accuracy * 100:.2f}%\")\n","    return predictions\n","\n","# Example preprocessing step: Convert text data into numerical format (TF-IDF)\n","def vectorize_text_data(train_data, test_data):\n","    vectorizer = TfidfVectorizer(max_features=1000)  # Limit to top 1000 features for performance\n","    X_train_tfidf = vectorizer.fit_transform(train_data)\n","    X_test_tfidf = vectorizer.transform(test_data)\n","    return X_train_tfidf, X_test_tfidf\n","\n","# Assume df_text is your DataFrame, X represents the feature column (text), and target_class represents the target column\n","X = 'cleaned_phrase'  # Replace with actual feature column name\n","target_class = 'prompt'  # Replace with actual target column\n","\n","# Split data\n","X_train_raw, X_test_raw, y_train, y_test = read_in_and_split_data(df_text, X, target_class)\n","\n","# Vectorize text data using TF-IDF\n","X_train, X_test = vectorize_text_data(X_train_raw, X_test_raw)\n","\n","# Define models to train\n","models = []\n","models.append(('Random Forest', RandomForestClassifier(n_estimators=100)))\n","models.append(('XGBoost', XGBClassifier()))\n","models.append(('Decision Tree', DecisionTreeClassifier()))\n","models.append(('Deep Learning', MLPClassifier(max_iter=500)))  # Simple deep learning\n","\n","# Train and evaluate other models using cross-validation\n","fit_model(X_train, y_train, models)\n","\n","# Train and evaluate Naive Bayes classifier separately\n","naive_bayes_predictions = naive_bayes_classifier(X_train, X_test, y_train, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8om4w4p-jnC3"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from xgboost import XGBClassifier\n","from sklearn.neural_network import MLPClassifier  # Simple deep learning (multi-layer perceptron)\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","# Function to fit and evaluate models\n","def fit_model(X_train, y_train, models):\n","    num_folds = 10\n","    scoring = 'accuracy'\n","\n","    results = []\n","    names = []\n","    for name, model in models:\n","        kfold = KFold(n_splits=num_folds)\n","\n","        # Convert y_train to numerical labels using LabelEncoder\n","        le = LabelEncoder()\n","        y_train_encoded = le.fit_transform(y_train)\n","\n","        cv_results = cross_val_score(model, X_train, y_train_encoded, cv=kfold, scoring=scoring)\n","        results.append(cv_results)\n","        names.append(name)\n","        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","        print(msg)\n","\n","# Split data to training and validation set\n","def read_in_and_split_data(data, features, target):\n","    X = data[features]\n","    y = data[target]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","    return X_train, X_test, y_train, y_test\n","\n","# Function to train and predict with Naive Bayes and print classification report\n","def naive_bayes_classifier(X_train, X_test, y_train, y_test):\n","    model = MultinomialNB()\n","    model.fit(X_train, y_train)  # Train the Naive Bayes model\n","    predictions = model.predict(X_test)  # Predict on the test set\n","\n","    # Evaluate accuracy\n","    accuracy = accuracy_score(y_test, predictions)\n","    print(f\"Naive Bayes Accuracy: {accuracy * 100:.2f}%\")\n","\n","    # Print classification report\n","    report = classification_report(y_test, predictions)\n","    print(\"Classification Report:\\n\", report)\n","\n","    return predictions\n","\n","# Example preprocessing step: Convert text data into numerical format (TF-IDF)\n","def vectorize_text_data(train_data, test_data):\n","    vectorizer = TfidfVectorizer(max_features=1000)  # Limit to top 1000 features for performance\n","    X_train_tfidf = vectorizer.fit_transform(train_data)\n","    X_test_tfidf = vectorizer.transform(test_data)\n","    return X_train_tfidf, X_test_tfidf\n","\n","# Assume df_text is your DataFrame, X represents the feature column (text), and target_class represents the target column\n","X = 'cleaned_phrase'  # Replace with actual feature column name\n","target_class = 'prompt'  # Replace with actual target column\n","\n","# Split data\n","X_train_raw, X_test_raw, y_train, y_test = read_in_and_split_data(df_text, X, target_class)\n","\n","# Vectorize text data using TF-IDF\n","X_train, X_test = vectorize_text_data(X_train_raw, X_test_raw)\n","\n","# Define models to train\n","models = []\n","models.append(('Random Forest', RandomForestClassifier(n_estimators=100)))\n","models.append(('XGBoost', XGBClassifier()))\n","models.append(('Decision Tree', DecisionTreeClassifier()))\n","models.append(('Deep Learning', MLPClassifier(max_iter=500)))  # Simple deep learning\n","\n","# Train and evaluate other models using cross-validation\n","fit_model(X_train, y_train, models)\n","\n","# Train and evaluate Naive Bayes classifier separately, including classification report\n","naive_bayes_predictions = naive_bayes_classifier(X_train, X_test, y_train, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQ2AlNvTjnC5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import label_binarize\n","\n","# Function to plot ROC curve and AUC\n","def plot_roc_curve(y_test, y_probs, model_name):\n","    fpr, tpr, _ = roc_curve(y_test, y_probs)  # Compute ROC curve\n","    roc_auc = auc(fpr, tpr)  # Compute AUC\n","\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Random chance line\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(f'Receiver Operating Characteristic: {model_name}')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","# Function to fit models and plot ROC curves with AUC\n","def fit_and_plot_roc(X_train, X_test, y_train, y_test, models):\n","    for name, model in models:\n","        model.fit(X_train, y_train)\n","\n","        # Predict probabilities or decision function for ROC (binary classification)\n","        if hasattr(model, \"predict_proba\"):\n","            y_probs = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n","        elif hasattr(model, \"decision_function\"):\n","            y_probs = model.decision_function(X_test)\n","        else:\n","            raise Exception(f'Model {name} does not have predict_proba or decision_function')\n","\n","        # Plot ROC curve and AUC\n","        plot_roc_curve(y_test, y_probs, name)\n","\n","# Example models (define your models)\n","# models = [('Logistic Regression', LogisticRegression()), ('Random Forest', RandomForestClassifier())]\n","\n","# Ensure binary targets for ROC (binarize if necessary)\n","if len(set(y_train)) > 2:\n","    y_train = label_binarize(y_train, classes=list(set(y_train)))[:, 0]  # Class 0 vs others\n","    y_test = label_binarize(y_test, classes=list(set(y_test)))[:, 0]\n","\n","# Train and plot ROC curve with AUC for multiple models\n","fit_and_plot_roc(X_train, X_test, y_train, y_test, models)\n","\n","# For Naive Bayes, get probabilities and plot ROC with AUC\n","naive_bayes_model = MultinomialNB()\n","naive_bayes_model.fit(X_train, y_train)\n","nb_probs = naive_bayes_model.predict_proba(X_test)[:, 1]  # Probability for class 1\n","plot_roc_curve(y_test, nb_probs, 'Naive Bayes')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9DSlEGVjnC6"},"outputs":[],"source":["!pip install scispacy\n","!pip install pysoundfile\n","!apt-get install libav-tools -y\n","!apt-get install zip"]},{"cell_type":"code","source":["from fastai.text import *\n","from fastai.vision import *\n","import spacy\n","from spacy import displacy\n","import scispacy\n","import librosa\n","import librosa.display\n","import soundfile as sf\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud, STOPWORDS\n","from collections import Counter\n","import IPython\n","import os\n","from glob import glob\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import pylab\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","%matplotlib inline"],"metadata":{"id":"-wBkZ1do5nEX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Defining Helper Functions**"],"metadata":{"id":"AyoWleMi5tKi"}},{"cell_type":"code","source":["def get_wav_info(wav_file):\n","    data, rate = sf.read(wav_file)\n","    return data, rate\n","\n","def create_spectrogram(wav_file):\n","    # adapted from Andrew Ng Deep Learning Specialization Course 5\n","    data, rate = get_wav_info(wav_file)\n","    nfft = 200 # Length of each window segment\n","    fs = 8000 # Sampling frequencies\n","    noverlap = 120 # Overlap between windows\n","    nchannels = data.ndim\n","    if nchannels == 1:\n","        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n","    elif nchannels == 2:\n","        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n","    return pxx\n","\n","def create_melspectrogram(filename,name):\n","\n","    plt.interactive(False)\n","    clip, sample_rate = librosa.load(filename, sr=None)\n","    fig = plt.figure(figsize=[0.72,0.72])\n","    ax = fig.add_subplot(111)\n","    ax.axes.get_xaxis().set_visible(False)\n","    ax.axes.get_yaxis().set_visible(False)\n","    ax.set_frame_on(False)\n","    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n","    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n","    filename  = Path('/content/drive/MyDrive/test/spectrograms/' + name + '.jpg')\n","    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n","    plt.close()\n","    fig.clf()\n","    plt.close(fig)\n","    plt.close('all')\n","    del filename,name,clip,sample_rate,fig,ax,S\n","\n","def wordBarGraphFunction(df,column,title):\n","\n","    topic_words = [ z.lower() for y in\n","                       [ x.split() for x in df[column] if isinstance(x, str)]\n","                       for z in y]\n","    word_count_dict = dict(Counter(topic_words))\n","    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n","    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n","    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n","    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n","    plt.title(title)\n","    plt.show()\n","\n","def wordCloudFunction(df,column,numWords):\n","    topic_words = [ z.lower() for y in\n","                       [ x.split() for x in df[column] if isinstance(x, str)]\n","                       for z in y]\n","    word_count_dict = dict(Counter(topic_words))\n","    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n","    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n","    word_string=str(popular_words_nonstop)\n","    wordcloud = WordCloud(stopwords=STOPWORDS,\n","                          background_color='white',\n","                          max_words=numWords,\n","                          width=1000,height=1000,\n","                         ).generate(word_string)\n","    plt.clf()\n","    plt.imshow(wordcloud)\n","    plt.axis('off')\n","    plt.show()"],"metadata":{"id":"Z3nqBZLO5uWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["overview = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/overview-of-recordings.csv')\n","overview = overview[['file_name','phrase','prompt','overall_quality_of_the_audio','speaker_id']]\n","overview=overview.dropna()\n","overviewAudio = overview[['file_name','prompt']]\n","overviewAudio['spec_name'] = overviewAudio['file_name'].str.rstrip('.wav')\n","overviewAudio = overviewAudio[['spec_name','prompt']]\n","overviewText = overview[['phrase','prompt']]\n","noNaNcsv = '/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/overview-of-recordings.csv'\n","noNaNcsv = pd.read_csv(noNaNcsv)\n","noNaNcsv = noNaNcsv.dropna()\n","noNaNcsv = noNaNcsv.to_csv('overview-of-recordings.csv',index=False)\n","noNaNcsv"],"metadata":{"id":"tJBgwdWH6L6w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data exploratory analysis and visualization.**"],"metadata":{"id":"dgeWqPcw6mVb"}},{"cell_type":"code","source":["overview[110:120]"],"metadata":{"id":"FGMbrqaj6nDv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The categories of ailments and the quality of the audio descriptions are described below**"],"metadata":{"id":"BGZBV3dJ60kb"}},{"cell_type":"code","source":["sns.set_style(\"whitegrid\")\n","promptsPlot = sns.countplot(y='prompt',data=overview)\n","promptsPlot\n","\n","qualityPlot = sns.FacetGrid(overview,aspect=2.5)\n","qualityPlot.map(sns.kdeplot,'overall_quality_of_the_audio',shade= True)\n","qualityPlot.set(xlim=(2.5, overview['overall_quality_of_the_audio'].max()))\n","qualityPlot.set_axis_labels('overall_quality_of_the_audio', 'Proportion')\n","qualityPlot"],"metadata":{"id":"J5m-Dqv861Vo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.stats as stats\n","\n","# QQ Plot\n","plt.figure(figsize=(4, 4))\n","stats.probplot(overview['overall_quality_of_the_audio'], dist=\"norm\", plot=plt)\n","plt.title('QQ Plot of Overall Quality of Audio')\n","plt.xlabel('Theoretical Quantiles')\n","plt.ylabel('Sample Quantiles')\n","plt.grid()\n","plt.show()"],"metadata":{"id":"HASYzOQc7Pob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Boxplot\n","plt.figure(figsize=(6, 4))\n","sns.boxplot(y='overall_quality_of_the_audio', data=overview)\n","plt.title('Boxplot of Overall Quality of Audio')\n","plt.ylabel('Overall Quality of Audio')\n","plt.grid()\n","plt.show()"],"metadata":{"id":"LGNAKtpc7S2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set the aesthetics for the plots\n","sns.set_style(\"whitegrid\")\n","\n","# Histogram\n","plt.figure(figsize=(6,4))\n","sns.histplot(overview['overall_quality_of_the_audio'], bins=30, kde=True)\n","plt.title('Histogram of Overall Quality of Audio')\n","plt.xlabel('Overall Quality of Audio')\n","plt.ylabel('Frequency')\n","plt.grid()\n","plt.show()"],"metadata":{"id":"RUfI5QJK7VY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["overview[62:63]"],"metadata":{"id":"s9COYgOg7YEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IPython.display.Audio('/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/test/1249120_20518958_23074828.wav')"],"metadata":{"id":"tG3IGi-z7bYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["overview[118:119]"],"metadata":{"id":"aluGSOpq7yQr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","# Download the stopwords dataset\n","nltk.download('stopwords')"],"metadata":{"id":"oSFDj8HF72gD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(6,6))\n","wordCloudFunction(overview,'phrase',10000000)"],"metadata":{"id":"YDilhjZz75WR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,10))\n","wordBarGraphFunction(overview,'phrase',\"Most Common Words in Medical Text Transcripts\")"],"metadata":{"id":"Mi4WMIUY7_Fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path"],"metadata":{"id":"cAGuhjtS8BvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install fastai"],"metadata":{"id":"0XV5TIAc8EIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fastai --upgrade"],"metadata":{"id":"DDmx8ClZ8HBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fastai.text import *\n","from fastai.vision import *"],"metadata":{"id":"lVyODWz88LCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fastai --upgrade"],"metadata":{"id":"6COdhGGl8Nlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","from fastai.text.all import * # Changed to import using the all wildcard\n","import numpy as np\n","from pathlib import Path\n","\n","# Set random seed\n","np.random.seed(7)\n","\n","# Define the path to your data\n","path = Path('/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/')\n","\n","# Load data using the newer DataBlock API\n","data_clas = TextDataLoaders.from_csv(path, 'overview-of-recordings.csv',\n","                                      cols='phrase',\n","                                      label_col='prompt',\n","                                      valid_pct=0.2,\n","                                      bs=42)\n","\n","# Set model path\n","MODEL_PATH = \"/tmp/model/\"\n","\n","# Create a text classifier learner\n","learn = text_classifier_learner(data_clas, model_dir=MODEL_PATH, arch=AWD_LSTM)\n","\n","# Fit the model\n","learn.fit_one_cycle(5)"],"metadata":{"id":"2qhuA5UB8SAm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learn.unfreeze()\n","learn.fit_one_cycle(5)"],"metadata":{"id":"OC7EGB57-Cpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["interp = ClassificationInterpretation.from_learner(learn)\n","interp.plot_confusion_matrix(figsize=(10,10), dpi=60)"],"metadata":{"id":"KOuGvpRb-Fe_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Part 3 of 3: Classify Ailment from Audio Description**\n"],"metadata":{"id":"mChWi7jE-KnA"}},{"cell_type":"code","source":["testAudio = \"/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/train/1249120_44176037_58635902.wav\"\n","x = create_spectrogram(testAudio)"],"metadata":{"id":"L6EXiGke-NQT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Mel-frequency_cepstrum Application.**\n"],"metadata":{"id":"-SNC28ue-UaX"}},{"cell_type":"code","source":["!pip uninstall scipy\n","!pip install scipy"],"metadata":{"id":"X5RTQgrcBk-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = \"/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/train/1249120_44176037_58635902.wav\"\n","clip, sample_rate = librosa.load(filename, sr=None)\n","fig = plt.figure(figsize=[5,5])\n","S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n","librosa.display.specshow(librosa.power_to_db(S, ref=np.max))"],"metadata":{"id":"0NgXWKnN-PNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from glob import glob"],"metadata":{"id":"-QgT4aEE-XYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cat /etc/issue\n"],"metadata":{"id":"QvmsUMpj-cid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from glob import glob\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error\n","from sklearn.ensemble import RandomForestClassifier  # Choose an appropriate model\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","\n","# Function to create mel spectrograms (Assuming you have this defined)\n","def create_melspectrogram(filename, name):\n","    # Load the audio file\n","    y, sr = librosa.load(filename)\n","    # Create a mel spectrogram\n","    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n","    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","\n","    # Flatten the mel spectrogram for feature extraction\n","    return mel_spectrogram_db.flatten()  # Flatten for later use\n","\n","# Define directories\n","data_dir_train = np.array(glob(\"/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/train/*\"))\n","data_dir_test = np.array(glob(\"/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/test/*\"))\n","data_dir_val = np.array(glob(\"/content/drive/MyDrive/Colab Notebooks/Data/Medical Speech, Transcription, and Intent/recordings/validate/*\"))\n","\n","# Create mel spectrograms and extract features\n","features = []\n","labels = []  # Add corresponding labels based on your dataset\n","\n","for file in tqdm(data_dir_train):\n","    filename, name = file, file.split('/')[-1].split('.')[0]\n","    mel_features = create_melspectrogram(filename, name)\n","    features.append(mel_features)\n","    labels.append('your_label_here')  # Replace with the actual label for each file\n","\n","# Repeat for test and validation sets if necessary\n","\n","# Prepare your data into a DataFrame\n","df = pd.DataFrame(features)\n","df['label'] = labels\n","\n","# Check the shape of the DataFrame\n","print(\"DataFrame shape:\", df.shape)\n","\n","# Ensure there are features and labels before proceeding\n","if df.empty or df['label'].isnull().all():\n","    raise ValueError(\"No data available for training. Check your feature extraction process.\")\n","\n","# Split the dataset into features and target\n","X = df.drop('label', axis=1)\n","y = df['label']\n","\n","# Check if there are enough samples\n","if len(X) == 0 or len(y) == 0:\n","    raise ValueError(\"Feature or target variable is empty.\")\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# Train a classifier\n","model = RandomForestClassifier()\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Generate classification report\n","report = classification_report(y_test, y_pred)\n","print(\"Classification Report:\\n\", report)\n","\n","# Calculate and print MAE, MSE, RMSE\n","# Convert predictions to numerical values for error metrics calculation\n","# If your labels are categorical, you can map them to numerical values\n","# For this example, we'll assume binary labels; adjust accordingly for your use case.\n","y_test_numeric = pd.factorize(y_test)[0]  # Convert to numerical\n","y_pred_numeric = pd.factorize(y_pred)[0]  # Convert to numerical\n","\n","mae = mean_absolute_error(y_test_numeric, y_pred_numeric)\n","mse = mean_squared_error(y_test_numeric, y_pred_numeric)\n","rmse = np.sqrt(mse)\n","\n","# Print metrics\n","print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n","print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n"],"metadata":{"id":"REUu41_H-dRC"},"execution_count":null,"outputs":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":125828,"sourceId":302713,"sourceType":"datasetVersion"},{"datasetId":569127,"sourceId":1032482,"sourceType":"datasetVersion"}],"dockerImageVersionId":30066,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4rc1"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}